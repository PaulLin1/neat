{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e62632-12f0-46ce-9d06-bd913a90f79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "top model loss: 4.05\n",
      "12\n",
      "epoch: 1\n",
      "top model loss: 3.85\n",
      "8\n",
      "epoch: 2\n",
      "top model loss: 3.25\n",
      "9\n",
      "epoch: 3\n",
      "top model loss: 3.07\n",
      "6\n",
      "epoch: 4\n",
      "top model loss: 3.20\n",
      "4\n",
      "epoch: 5\n",
      "top model loss: 3.19\n",
      "3\n",
      "epoch: 6\n",
      "top model loss: 3.09\n",
      "4\n",
      "epoch: 7\n",
      "top model loss: 2.75\n",
      "2\n",
      "epoch: 8\n",
      "top model loss: 2.75\n",
      "1\n",
      "epoch: 9\n",
      "top model loss: 2.74\n",
      "2\n",
      "epoch: 10\n",
      "top model loss: 2.74\n",
      "2\n",
      "epoch: 11\n",
      "top model loss: 2.74\n",
      "1\n",
      "epoch: 12\n",
      "top model loss: 2.62\n",
      "1\n",
      "epoch: 13\n",
      "top model loss: 2.61\n",
      "1\n",
      "epoch: 14\n",
      "top model loss: 2.23\n",
      "1\n",
      "epoch: 15\n",
      "top model loss: 2.21\n",
      "1\n",
      "epoch: 16\n",
      "top model loss: 2.21\n",
      "1\n",
      "epoch: 17\n",
      "top model loss: 2.19\n",
      "1\n",
      "epoch: 18\n",
      "top model loss: 2.19\n",
      "1\n",
      "epoch: 19\n",
      "top model loss: 2.12\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from neat.cppn import *\n",
    "from neat.genome import *\n",
    "from neat.speciation import *\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# sklearn digits\n",
    "\n",
    "# 8x8\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "data_tensor = torch.tensor(digits.data, dtype=torch.float32)\n",
    "data_tensor = torch.tensor(digits.data / 16.0, dtype=torch.float32) # Normalize for neat\n",
    "target_tensor = torch.tensor(digits.target, dtype=torch.long)\n",
    "\n",
    "# 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_tensor, target_tensor, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Batch is the full size because there is no backpropogation\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# MNIST\n",
    "\n",
    "# 28x28\n",
    "\n",
    "# from torchvision import datasets, transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),  # Converts images to PyTorch tensors\n",
    "#     transforms.Normalize((0.1307,), (0.3081,))  # Mean and std dev for MNIST\n",
    "# ])\n",
    "\n",
    "# # Load training and test datasets\n",
    "# train_dataset = datasets.MNIST(root='./data', train=True, transform=transform)\n",
    "# test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "# # Subset stuff\n",
    "# train_size = len(train_dataset) // 4\n",
    "# test_size = len(test_dataset) // 4\n",
    "\n",
    "# train_subset = Subset(train_dataset, torch.randperm(len(train_dataset))[:train_size])\n",
    "# test_subset = Subset(test_dataset, torch.randperm(len(test_dataset))[:test_size])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=len(train_dataset), shuffle=False)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Init stuff\n",
    "\n",
    "# Hyperparameters\n",
    "population_size = 300\n",
    "epochs = 300\n",
    "input_dim = 8*8\n",
    "output_dim = 10\n",
    "top_k = 0.4 # The percentage of genomes to keep for reproduction\n",
    "crossover_percent = 0.5\n",
    "\n",
    "# hyperparameters for measuring compatibility from https://nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf\n",
    "c1 = 1.0\n",
    "c2 = 1.0\n",
    "c3 = 3.0\n",
    "delta_thresh = 3.4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Using list of lists\n",
    "# Dead species will not be kept track of. There will be no empty list\n",
    "population = []\n",
    "\n",
    "# Reset NN Class\n",
    "reset_NN_class_state()\n",
    "\n",
    "# Init first model\n",
    "new_model = {\"model\": NN(input_dim, output_dim).to(device), \"loss\": float('inf'), \"fitness\": -float('inf')}\n",
    "population.append([new_model])\n",
    "\n",
    "for _ in range(population_size - 1):\n",
    "    new_model = {\"model\": NN(input_dim, output_dim).to(device), \"loss\": float('inf'), \"fitness\": -float('inf')}\n",
    "    \n",
    "    added = False\n",
    "    for idx, species in enumerate(population):\n",
    "        delta = measure_compatibility(new_model['model'], species[0]['model'], c1, c2, c3)\n",
    "\n",
    "        if delta < delta_thresh:\n",
    "            population[idx].append(new_model)\n",
    "            added = True\n",
    "            break\n",
    "    if not added:\n",
    "        # New species created\n",
    "        population.append([new_model])\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# \"Training\" loop\n",
    "\n",
    "for epoch in range(epochs):  \n",
    "    for species in population:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for model_info in species:\n",
    "                model_info[\"model\"] = model_info[\"model\"].to(device)\n",
    "                model = model_info[\"model\"]\n",
    "                total_loss = 0.0\n",
    "                total_samples = 0\n",
    "    \n",
    "                for data_batch, label_batch in train_loader:\n",
    "                    data_batch = data_batch.to(device)\n",
    "                    label_batch = label_batch.to(device)\n",
    "\n",
    "                    output = model(data_batch)\n",
    "                    loss = loss_fn(output, label_batch)\n",
    "                    total_loss += loss.item() * data_batch.size(0)\n",
    "                    total_samples += data_batch.size(0)\n",
    "                \n",
    "                model_info[\"loss\"] = total_loss / total_samples\n",
    "\n",
    "    flattened_population = []\n",
    "\n",
    "    for species in population:\n",
    "        for genome in species:\n",
    "            flattened_population.append(genome)\n",
    "            \n",
    "    ranked_models = sorted([model_info for model_info in flattened_population], key=lambda x: x[\"loss\"])\n",
    "    lowest_loss = ranked_models[0]['loss']\n",
    "\n",
    "    # Fitness sharing\n",
    "    for species in population:\n",
    "        species_size = len(species)\n",
    "        for genome in species:\n",
    "            raw_fitness = 1 / (1 + genome['loss'])\n",
    "            genome['fitness'] = raw_fitness / species_size\n",
    "\n",
    "    # Last epoch do not make new models\n",
    "    if epoch == epochs - 1:\n",
    "        break\n",
    "\n",
    "    # This is just a list not a list of lists\n",
    "    new_population = []\n",
    "\n",
    "    for species in population:\n",
    "        offspring = []\n",
    "\n",
    "        ranked_models = sorted([model_info for model_info in species], key=lambda x: x[\"fitness\"], reverse=True)\n",
    "        parents = [model_info for model_info in ranked_models[:math.ceil(top_k * len(ranked_models))]]\n",
    "\n",
    "        for i in range(math.ceil(crossover_percent * len(ranked_models))):\n",
    "            p1 = random.choice(parents)\n",
    "            p2 = random.choice(parents)\n",
    "            child = crossover(p1, p2)\n",
    "            offspring.append({\"model\": child.to(device), \"loss\": float('inf'), \"fitness\": -float('inf')})\n",
    "    \n",
    "        while len(offspring) != len(ranked_models):\n",
    "            offspring.append({\"model\": random.choice(parents)['model'].mutate(True).to(device), \"loss\": float('inf'), \"fitness\": -float('inf')})\n",
    "            \n",
    "        new_population.extend(offspring)\n",
    "\n",
    "    # Redivide into species\n",
    "    new_population_divided = []\n",
    "\n",
    "    for model in new_population:    \n",
    "        # First model\n",
    "        if len(new_population_divided) == 0:\n",
    "            new_population_divided.append([model])\n",
    "        else:\n",
    "            added = False\n",
    "            for idx, species in enumerate(new_population_divided):\n",
    "                delta = measure_compatibility(model['model'], species[0]['model'], c1, c2, c3)\n",
    "\n",
    "                if delta < delta_thresh:\n",
    "                    new_population_divided[idx].append(model)\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                # New species created\n",
    "                new_population_divided.append([model])\n",
    "                    \n",
    "    population = new_population_divided\n",
    "\n",
    "    # To keep track of the num of species per epoch\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    print(f\"top model loss: {lowest_loss:.2f}\")\n",
    "    print(len(population))\n",
    "\n",
    "model = ranked_models[0]['model'].to(device)\n",
    "\n",
    "# Evaulate\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(data)  # logits\n",
    "        predicted = torch.argmax(outputs, dim=1)  # class indices\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), f\"models/sklearn_digits_300pop_300epoch_real{accuracy * 100:.2f}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13110d4d-8b88-4bb9-98b5-ad748b742071",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"models/sklearn_digits_300pop_300epoch_real{accuracy * 100:.2f}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neat",
   "language": "python",
   "name": "neat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
