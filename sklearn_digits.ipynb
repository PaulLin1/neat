{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "435996a1-f11a-4189-91fa-b227beb18c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4963a086-7e38-438a-b65e-b1f960747457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(digits.data.shape)\n",
    "print(digits.target[44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01c32047-eeed-4b98-b75a-4718a09fa911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ef0358e-d2fc-4268-bb94-603ed55c71e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.Tensor(digits.data)\n",
    "target_tensor = F.one_hot(torch.Tensor(digits.target).long(), 10).float()\n",
    "\n",
    "dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "# Create dataloader\n",
    "batch_size = 64\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488f3861-4346-40a4-8538-3c6e41c555cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple FFN approach\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(8*8, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.flatten(x)\n",
    "        logits = linear_relu_stack(x)\n",
    "        return logits\n",
    "        return F.softmax(logits, dim=0)\n",
    "\n",
    "model = FFN()\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(0, len(data_tensor), batch_size):\n",
    "        pred = model(data_tensor[j:j+batch_size])\n",
    "        \n",
    "        target = target_tensor[j:j+batch_size]\n",
    "\n",
    "        loss = loss_fn(pred, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "# Double check\n",
    "print(torch.argmax(model(data_tensor[5])))\n",
    "print(target_tensor[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765f546-52dd-41fb-ba34-567b2257cbad",
   "metadata": {},
   "source": [
    "NEAT PYTORCH\n",
    "\n",
    "to do\n",
    "genome species population stuff\n",
    "crossover\n",
    "improve parallelization i think\n",
    "\n",
    "Parallelize the eval of each nn. The nn is not parallelizable because mm cannot be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1176c120-0522-440f-945d-0848012d838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEAT Classes\n",
    "\n",
    "from collections import deque, defaultdict\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, id_, input_=False, output=False):\n",
    "        self.id = id_\n",
    "        self.is_input = input_\n",
    "        self.is_output = output\n",
    "\n",
    "        self.val = torch.Tensor(0) # Tensors\n",
    "        self.num_incoming_connections = 0\n",
    "        \n",
    "        if not input_ and not output:\n",
    "            # Output has special handling when init because it starts off fully connected\n",
    "            self.num_incoming_connections = 1\n",
    "            \n",
    "        self.received = 0 # Keep track of nodes received before applying activation function\n",
    "        \n",
    "class ConnectionGene:\n",
    "    def __init__(self, in_node, out_node, weight, innov_num):\n",
    "        self.in_node = in_node\n",
    "        self.out_node = out_node\n",
    "        self.weight = weight # Weights are tensors\n",
    "        self.innov_num = innov_num\n",
    "\n",
    "        self.enable = True # If node is disabled, it CAN be reenabled\n",
    "\n",
    "\n",
    "\n",
    "class NN(nn.Module):\n",
    "    glob_node_num = 0\n",
    "    glob_innov_num = 0\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, crossover=False):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        if crossover:\n",
    "            # Initialized in a different way\n",
    "            self.input_dim = input_dim\n",
    "            self.output_dim = output_dim\n",
    "            \n",
    "            self.nodes = []\n",
    "            \n",
    "            self.connections = []\n",
    "            self.num_disabled = 0 # Might not need this anymore     \n",
    "\n",
    "        \n",
    "        else:\n",
    "            # When init a new model, the node nums and innov should be the same as any other new initialized model\n",
    "            local_node_num = 0\n",
    "            local_innov_num = 0\n",
    "    \n",
    "            self.input_dim = input_dim\n",
    "            self.output_dim = output_dim\n",
    "    \n",
    "            self.nodes = []\n",
    "            \n",
    "            self.connections = []\n",
    "            self.num_disabled = 0 # Might not need this anymore\n",
    "    \n",
    "            # Initalize a fully connected NN with no hidden layers\n",
    "            for _ in range(input_dim):\n",
    "                new_node = Node(local_node_num, True)\n",
    "                self.nodes.append(new_node)\n",
    "                local_node_num += 1\n",
    "    \n",
    "            for _ in range(output_dim):\n",
    "                new_node = Node(local_node_num, False, True)\n",
    "                self.nodes.append(new_node)\n",
    "                local_node_num += 1\n",
    "    \n",
    "                for i in range(input_dim):\n",
    "                    conn = ConnectionGene(self.nodes[i], new_node, torch.randn(1), local_innov_num)\n",
    "                    self.connections.append(conn)\n",
    "                    local_innov_num += 1\n",
    "                    new_node.num_incoming_connections += 1 # Each output starts off fully connected to input\n",
    "    \n",
    "            NN.glob_node_num = local_node_num\n",
    "            NN.glob_innov_num = local_innov_num\n",
    "\n",
    "    def reset(self):\n",
    "        for i in self.nodes:\n",
    "            i.val = 0\n",
    "            i.received = 0\n",
    "\n",
    "    def clone(self):\n",
    "        # Create a new NN instance with same input/output dims\n",
    "        new_nn = NN(self.input_dim, self.output_dim)\n",
    "\n",
    "        new_nn.num_disabled = self.num_disabled\n",
    "        \n",
    "        # Deep copy nodes\n",
    "        new_nn.nodes = []\n",
    "        for node in self.nodes:\n",
    "            new_node = Node(node.id, node.is_input, node.is_output)\n",
    "            new_node.val = node.val\n",
    "            new_node.num_incoming_connections = node.num_incoming_connections\n",
    "            new_node.received = node.received\n",
    "            new_nn.nodes.append(new_node)\n",
    "\n",
    "        # Deep copy connections\n",
    "        new_nn.connections = []\n",
    "        for conn in self.connections:\n",
    "            # Find the corresponding new nodes by id\n",
    "            in_node = next(n for n in new_nn.nodes if n.id == conn.in_node.id)\n",
    "            out_node = next(n for n in new_nn.nodes if n.id == conn.out_node.id)\n",
    "\n",
    "            new_conn = ConnectionGene(in_node, out_node, conn.weight, conn.innov_num)\n",
    "            new_conn.enable = conn.enable\n",
    "            new_nn.connections.append(new_conn)\n",
    "\n",
    "        return new_nn\n",
    "        \n",
    "    # def forward(self, x):\n",
    "    #     if len(x) != self.input_dim:\n",
    "    #         raise ValueError(\"Input dim is not correct\")\n",
    "    \n",
    "    #     self.reset()\n",
    "    \n",
    "    #     # Set input node values\n",
    "    #     for idx, val in enumerate(x):\n",
    "    #         self.nodes[idx].val = val\n",
    "    \n",
    "    #     queue = deque()\n",
    "    #     for node in self.nodes:\n",
    "    #         if node.received == node.num_incoming_connections:\n",
    "    #             queue.append(node)\n",
    "    \n",
    "    #     while queue:\n",
    "    #         curr_node = queue.popleft()\n",
    "    \n",
    "    #         for conn in self.connections:\n",
    "    #             if not conn.enable:\n",
    "    #                 continue\n",
    "    #             if conn.in_node != curr_node:\n",
    "    #                 continue\n",
    "    \n",
    "    #             out_node = conn.out_node\n",
    "    #             out_node.val += curr_node.val * conn.weight\n",
    "    #             out_node.received += 1\n",
    "    \n",
    "    #             if out_node.received == out_node.num_incoming_connections:\n",
    "    #                 if not out_node.is_output:\n",
    "    #                     out_node.val = max(0, out_node.val)  # ReLU\n",
    "    #                 queue.append(out_node)\n",
    "    \n",
    "    #     logits = torch.tensor([n.val for n in self.nodes if n.is_output])\n",
    "    #     #     # Cross entropy loss provides softmax so just returning logits\n",
    "    #     return logits\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if x.shape[1] != self.input_dim:\n",
    "            raise ValueError(\"Input dim is not correct\")\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        self.reset()\n",
    "    \n",
    "        # Create batch versions of node values and received counts\n",
    "        for node in self.nodes:\n",
    "            node.vals = torch.zeros(batch_size)\n",
    "            node.received = torch.zeros(batch_size, dtype=torch.int)\n",
    "    \n",
    "        # Set input values\n",
    "        for idx in range(self.input_dim):\n",
    "            self.nodes[idx].vals = x[:, idx]\n",
    "    \n",
    "        # Start with nodes whose incoming connections are already satisfied\n",
    "        queue = deque()\n",
    "        for node in self.nodes:\n",
    "            if node.num_incoming_connections == 0:\n",
    "                queue.append(node)\n",
    "    \n",
    "        while queue:\n",
    "            curr_node = queue.popleft()\n",
    "    \n",
    "            for conn in self.connections:\n",
    "                if not conn.enable:\n",
    "                    continue\n",
    "                if conn.in_node != curr_node:\n",
    "                    continue\n",
    "    \n",
    "                out_node = conn.out_node\n",
    "                out_node.vals += curr_node.vals * conn.weight\n",
    "                out_node.received += 1\n",
    "    \n",
    "                # Only enqueue if all inputs are received\n",
    "                # Note: vectorized check — adds node to queue if all samples are ready\n",
    "                if (out_node.received == out_node.num_incoming_connections).all():\n",
    "                    if not out_node.is_output:\n",
    "                        out_node.vals = torch.relu(out_node.vals)\n",
    "                    queue.append(out_node)\n",
    "    \n",
    "        # Collect logits from output nodes\n",
    "        output_vals = [node.vals for node in self.nodes if node.is_output]\n",
    "        logits = torch.stack(output_vals, dim=1)  # shape: (batch_size, num_outputs)\n",
    "        return logits\n",
    "\n",
    "    \n",
    "    def weight_perturbation(self, quiet):\n",
    "        rand_conn_id = torch.randint(0, len(self.connections), (1,)).item()\n",
    "        \n",
    "        mean = 0.0\n",
    "        std_dev = 0.1\n",
    "        \n",
    "        noise = torch.randn_like(self.connections[rand_conn_id].weight) * std_dev + mean\n",
    "        self.connections[rand_conn_id].weight += noise\n",
    "        \n",
    "        if not quiet:\n",
    "            print(f'connection {rand_conn_id} weight perturbated to {self.connections[rand_conn_id].weight.item():.2f}')\n",
    "        \n",
    "    def weight_modification(self, quiet):\n",
    "        rand_conn_id = torch.randint(0, len(self.connections), (1,)).item()\n",
    "\n",
    "        self.connections[rand_conn_id].weight = torch.randn(1)\n",
    "        \n",
    "        if not quiet:\n",
    "            print(f'connection {rand_conn_id} weight modified to {self.connections[rand_conn_id].weight.item():.2f}')\n",
    "\n",
    "    def add_connection(self, quiet):\n",
    "        max_attempts = 100  # prevent infinite loop\n",
    "        for _ in range(max_attempts):\n",
    "            rand_node_in = self.nodes[torch.randint(0, len(self.nodes), (1,)).item()]\n",
    "        \n",
    "            if rand_node_in.is_output:\n",
    "                continue\n",
    "        \n",
    "            rand_node_out = self.nodes[torch.randint(0, len(self.nodes), (1,)).item()]\n",
    "        \n",
    "            if rand_node_out == rand_node_in or rand_node_out.is_input:\n",
    "                continue\n",
    "        \n",
    "            # Skip if connection already exists\n",
    "            if any(conn.in_node == rand_node_in and conn.out_node == rand_node_out for conn in self.connections):\n",
    "                continue\n",
    "        \n",
    "            # Optional: skip if this would form a cycle\n",
    "            # if self.creates_cycle(rand_node_in, rand_node_out):\n",
    "            #     continue\n",
    "        \n",
    "            conn = ConnectionGene(rand_node_in, rand_node_out, torch.randn(1), NN.glob_innov_num)\n",
    "            self.connections.append(conn)\n",
    "            NN.glob_innov_num += 1\n",
    "            rand_node_out.num_incoming_connections += 1\n",
    "            \n",
    "            if not quiet:\n",
    "                print(f\"Connection created from node {rand_node_in.id} to node {rand_node_out.id} (innovation #{NN.glob_innov_num})\")\n",
    "\n",
    "            return\n",
    "            \n",
    "        if not quiet:\n",
    "            print(\"Failed to add connection after max attempts.\")\n",
    "\n",
    "    def add_node(self, quiet):            \n",
    "        rand_conn_id = torch.randint(0, len(self.connections), (1,)).item()\n",
    "\n",
    "        while not self.connections[rand_conn_id].enable:\n",
    "            rand_conn_id = torch.randint(0, len(self.connections), (1,)).item()\n",
    "\n",
    "        # Splits an existing connection by adding a node\n",
    "        self.connections[rand_conn_id].enable = False\n",
    "        self.num_disabled += 1\n",
    "        \n",
    "        new_node = Node(NN.glob_node_num)\n",
    "        self.nodes.append(new_node)\n",
    "        NN.glob_node_num += 1\n",
    "        \n",
    "        conn = ConnectionGene(self.connections[rand_conn_id].in_node, new_node, torch.randn(1), NN.glob_innov_num)\n",
    "        self.connections.append(conn)\n",
    "        NN.glob_innov_num += 1\n",
    "        \n",
    "        conn = ConnectionGene(new_node, self.connections[rand_conn_id].out_node, torch.randn(1), NN.glob_innov_num)\n",
    "        self.connections.append(conn)\n",
    "        NN.glob_innov_num += 1\n",
    "        \n",
    "        if not quiet:\n",
    "            print(f'connection {rand_conn_id} split')\n",
    "\n",
    "    def toggle_connection(self, quiet):\n",
    "        rand_conn_id = torch.randint(0, len(self.connections), (1,)).item()\n",
    "\n",
    "        if self.connections[rand_conn_id].enable:\n",
    "            self.num_disabled -= 1\n",
    "        else:\n",
    "            self.num_disabled += 1\n",
    "        self.connections[rand_conn_id].enable = not self.connections[rand_conn_id].enable\n",
    "\n",
    "        if not quiet:\n",
    "            print(f'connection {rand_conn_id} toggled to {self.connections[rand_conn_id].enable}')\n",
    "\n",
    "    def mutate(self, quiet=False):\n",
    "        # For this experiment i use 60 weight perturbation, 60 weight mutation, 5 add connection, 3 add node, 2 toggle\n",
    "        # Each one is chosen independently of each other\n",
    "        # Does not include crossover. That cannot be done by itself\n",
    "        new_model = self.clone()\n",
    "\n",
    "        # Do mutations on new_model\n",
    "        if random.random() < 0.60:\n",
    "            new_model.weight_perturbation(quiet)\n",
    "        if random.random() < 0.60:\n",
    "            new_model.weight_modification(quiet)\n",
    "        if random.random() < 0.05:\n",
    "            new_model.add_connection(quiet)\n",
    "        if random.random() < 0.03:\n",
    "            new_model.add_node(quiet)\n",
    "        if random.random() < 0.02:\n",
    "            new_model.toggle_connection(quiet)\n",
    "    \n",
    "        return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a08c669d-7c2d-4b73-993e-c2369ef3a35d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 13 (655297904.py, line 14)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[229]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn new_model\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'for' statement on line 13\n"
     ]
    }
   ],
   "source": [
    "def crossover(info1, info2):\n",
    "    # Equal Fitness\n",
    "    # Might not implement this rn because it doesnt happen that much\n",
    "    if info1['loss'] == info2['loss']:\n",
    "        pass\n",
    "\n",
    "    # Find fitter model\n",
    "    fit_model, less_fit_model = (info2['model'], info1['model']) if info1['loss'] > info2['loss'] else (info1['model'], info2['model'])\n",
    "\n",
    "    # New model starts off as clone of more fit\n",
    "    new_model = NN(fit_model.input_dim, fit_model.output_dim, True)\n",
    "\n",
    "    fit_conn_pointer = less_fit_conn_pointer = 0\n",
    "\n",
    "    while fit_conn_pointer != len(fit_model.connections) - 1 and less_fit_conn_pointer != len(less_fit_model.connections) - 1:\n",
    "        # IDK\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f53ddb4b-c42d-4bb9-aab8-31764af7ab97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters for measuring compatibility from https://nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf\n",
    "c1 = 1.0\n",
    "c2 = 1.0\n",
    "c3 = 3.0\n",
    "delta_thresh = 4.0\n",
    "\n",
    "def measure_compatibility(genome1, genome2):\n",
    "    genome1_conns = {i.innov_num: i.weight for i in genome1.connections}\n",
    "    genome2_conns = {i.innov_num: i.weight for i in genome2.connections}\n",
    "\n",
    "    innovs1 = set(genome1_conns.keys())\n",
    "    innovs2 = set(genome2_conns.keys())\n",
    "\n",
    "    max_innov1 = max(innovs1) if innovs1 else 0\n",
    "    max_innov2 = max(innovs2) if innovs2 else 0\n",
    "    max_innov = max(max_innov1, max_innov2)\n",
    "\n",
    "    # Matching genes: innovation numbers in both genomes\n",
    "    matching = innovs1.intersection(innovs2)\n",
    "    # Calculate average weight difference for matching genes\n",
    "    if matching:\n",
    "        weight_diff = sum(abs(genome1_conns[i] - genome2_conns[i]) for i in matching) / len(matching)\n",
    "    else:\n",
    "        weight_diff = 0\n",
    "\n",
    "    # Excess genes: genes whose innovation number is greater than max innovation number of other genome\n",
    "    excess = 0\n",
    "    for innov in innovs1:\n",
    "        if innov > max_innov2:\n",
    "            excess += 1\n",
    "    for innov in innovs2:\n",
    "        if innov > max_innov1:\n",
    "            excess += 1\n",
    "\n",
    "    # Disjoint genes: genes that do not match and are not excess\n",
    "    disjoint = (len(innovs1 - innovs2) + len(innovs2 - innovs1)) - excess\n",
    "\n",
    "    # Normalization factor N\n",
    "    N = max(len(genome1_conns), len(genome2_conns))\n",
    "    if N < 20:\n",
    "        N = 1  # as per original NEAT paper for small genomes\n",
    "\n",
    "    delta = (c1 * excess / N) + (c2 * disjoint / N) + (c3 * weight_diff)\n",
    "    \n",
    "    # return delta\n",
    "    return True if delta < delta_thresh else False\n",
    "\n",
    "m1 = NN(64, 10)\n",
    "m2 = NN(64, 10)\n",
    "\n",
    "measure_compatibility(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa873000-3f48-41af-a8d7-11832bb03591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection 11 weight modified to 1.86\n",
      "connection 398 weight perturbated to 0.97\n",
      "connection 615 toggled to False\n",
      "connection 292 weight perturbated to 0.15\n",
      "connection 212 weight perturbated to -0.21\n",
      "connection 136 weight modified to 0.38\n",
      "connection 121 weight modified to -1.29\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "model_infos = [{\"model\": NN(64, 10), \"loss\": float('inf')} for _ in range(10)]\n",
    "top_models = [info[\"model\"] for info in model_infos[:5]]\n",
    "\n",
    "for model in top_models:\n",
    "        model_infos.append({\"model\": model.mutate(), \"loss\": float('inf')})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27876d56-916c-4695-b0b7-9cb94b94bd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top model loss: 63.74\n",
      "top model loss: 62.59\n",
      "top model loss: 62.54\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m total_samples = \u001b[32m0\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data_batch, label_batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     loss = loss_fn(output, label_batch)\n\u001b[32m     28\u001b[39m     total_loss += loss.item() * data_batch.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neuroevol/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neuroevol/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 192\u001b[39m, in \u001b[36mNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    188\u001b[39m out_node.received += \u001b[32m1\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# Only enqueue if all inputs are received\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# Note: vectorized check — adds node to queue if all samples are ready\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mout_node\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreceived\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_node\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_incoming_connections\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_node.is_output:\n\u001b[32m    194\u001b[39m         out_node.vals = torch.relu(out_node.vals)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# \"Training loop\"\n",
    "\n",
    "# Hyperparameters\n",
    "num_models_init = 20\n",
    "epochs = 100\n",
    "num_sample_trained_on = 1000\n",
    "input_dim = 64\n",
    "output_dim = 10\n",
    "\n",
    "# Init\n",
    "model_infos = [{\"model\": NN(input_dim, output_dim), \"loss\": float('inf')} for _ in range(num_models_init)]\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Potentially can have mutate percents as hyperparameters here. Lowkey dont want to refactor rn\n",
    "\n",
    "for i in range(epochs):\n",
    "    for info in model_infos:\n",
    "        info[\"model\"].reset()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for info in model_infos:\n",
    "            model = info[\"model\"]\n",
    "            total_loss = 0.0\n",
    "            total_samples = 0\n",
    "\n",
    "            for data_batch, label_batch in loader:\n",
    "                output = model(data_batch)\n",
    "                loss = loss_fn(output, label_batch)\n",
    "                total_loss += loss.item() * data_batch.size(0)\n",
    "                total_samples += data_batch.size(0)\n",
    "            \n",
    "            info[\"loss\"] = total_loss / total_samples\n",
    "\n",
    "    # Especiation stuff goes here before you rank the models\n",
    "    \n",
    "    ranked_models = sorted(model_infos, key=lambda x: x[\"loss\"])\n",
    "    print(f\"top model loss: {ranked_models[0]['loss']:.2f}\")\n",
    "\n",
    "    top_models = [info for info in ranked_models[:int(num_models_init/2)]]\n",
    "\n",
    "\n",
    "    # for model in top_models:\n",
    "    #     model_infos.append({\"model\": model[\"model\"].mutate(), \"loss\": float('inf')})\n",
    "\n",
    "    # Test removing old models for faster performance and only mutating top\n",
    "    \n",
    "    new_models = [{\"model\": ranked_models[0]['model'].mutate(True), \"loss\": float('inf')} for _ in range(len(top_models))]\n",
    "    # new_models = [{\"model\": model[\"model\"].mutate(True), \"loss\": float('inf')} for model in top_models]\n",
    "    model_infos = top_models + new_models\n",
    "\n",
    "        \n",
    "\n",
    "# And then mutate the top models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a3d1afc7-bc50-4463-a2e9-1dd53fa81dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(64, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5454f1fd-ee56-4bdc-81c8-b1c5b2fd6566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -8.2188, -15.4980, -83.0724,  33.3792, -99.2694,   3.3089,  24.0452,\n",
       "        -37.0231, -81.9885,  47.4248])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fa66cc0e-53b7-49d5-ab65-9d64d27e09d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection 597 weight perturbated to 1.89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NN()"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.mutate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4e3e0-9bbb-4024-a506-dd98805d749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ca278-7301-4bbc-b224-0591355faeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(data_tensor[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroevol",
   "language": "python",
   "name": "neuroevol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
