{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "435996a1-f11a-4189-91fa-b227beb18c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "print(digits.data.shape)\n",
    "print(digits.target[44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01c32047-eeed-4b98-b75a-4718a09fa911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ef0358e-d2fc-4268-bb94-603ed55c71e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.Tensor(digits.data)\n",
    "target_tensor = F.one_hot(torch.Tensor(digits.target).long(), 10).float()\n",
    "\n",
    "dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "# Create dataloader\n",
    "# Wait we are not doing backpropogation so cant we just make batch_size the entire thing?\n",
    "batch_size = 1797\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765f546-52dd-41fb-ba34-567b2257cbad",
   "metadata": {},
   "source": [
    "NEAT PYTORCH\n",
    "\n",
    "to do\n",
    "genome species population stuff\n",
    "crossover\n",
    "improve parallelization i think\n",
    "\n",
    "Parallelize the eval of each nn. The nn is not parallelizable because mm cannot be used. \n",
    "\n",
    "\n",
    "rn every new connection gets an innovation number. this is wrong. you should check if they exist. if they exists you have to copy that innov number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1176c120-0522-440f-945d-0848012d838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEAT Classes\n",
    "\n",
    "from collections import deque, defaultdict\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, id_, input_=False, output=False):\n",
    "        self.id = id_\n",
    "        self.is_input = input_\n",
    "        self.is_output = output\n",
    "\n",
    "        self.val = torch.Tensor(0) # Tensors\n",
    "        self.num_incoming_connections = 0\n",
    "        \n",
    "        if not input_ and not output:\n",
    "            # Output has special handling when init because it starts off fully connected\n",
    "            # These are nodes that are created from splitting connection\n",
    "            self.num_incoming_connections = 1\n",
    "            \n",
    "        self.received = 0 # Keep track of nodes received before applying activation function\n",
    "        \n",
    "class ConnectionGene:\n",
    "    def __init__(self, in_node, out_node, innov_num, weight):\n",
    "        self.in_node = in_node\n",
    "        self.out_node = out_node\n",
    "        self.innov_num = innov_num\n",
    "        \n",
    "        self.weight = weight # Weights are tensors\n",
    "        self.enable = True # If node is disabled, it CAN be reenabled\n",
    "\n",
    "        self.resulting_node = None # If this connection is split, what is the resulting node\n",
    "\n",
    "class NN(nn.Module):\n",
    "    global_nodes = []\n",
    "\n",
    "    # Existing connections\n",
    "    # key: tuple of in and out node\n",
    "    # value: connection object\n",
    "    # Index is innov_number\n",
    "    # Now that we use this schema, might have to do some crazy refactoring\n",
    "    global_connections = {}\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, mutate=False, crossover=False):\n",
    "        super(NN, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.nodes = [] # Nodes in this specific NN\n",
    "        self.connections = [] # Connections in this specific NN\n",
    "\n",
    "        # Mutated models should not be inited!\n",
    "        if mutate:\n",
    "            return\n",
    "        elif crossover:\n",
    "            # Initialized in a different way\n",
    "            self.input_dim = input_dim\n",
    "            self.output_dim = output_dim\n",
    "            \n",
    "            self.nodes = []\n",
    "            \n",
    "            self.connections = []\n",
    "        else:\n",
    "            # When init a new model, the node nums and innov should be the same as any other new initialized model\n",
    "            # This else only gets models in the initial population\n",
    "            # Initalize a fully connected NN with no hidden layers\n",
    "            for i in range(input_dim):\n",
    "                if len(NN.global_nodes) <= i:\n",
    "                    new_node = Node(True)\n",
    "                    NN.global_nodes.append(new_node)\n",
    "                self.nodes.append(i)\n",
    "    \n",
    "            for i in range(output_dim):\n",
    "                node_index = i + input_dim\n",
    "                if len(NN.global_nodes) <= node_index:\n",
    "                    new_node = Node(False, True)\n",
    "                    NN.global_nodes.append(new_node)\n",
    "                self.nodes.append(node_index)\n",
    "\n",
    "                for j in range(input_dim):\n",
    "                    if (j, node_index) not in NN.global_connections.keys():\n",
    "                        conn = ConnectionGene(NN.global_nodes[j], NN.global_nodes[node_index], torch.randn(1))\n",
    "                        NN.global_connections.update({(j, node_index): conn})\n",
    "                    self.connections.append(list(NN.global_connections.keys()).index((j, node_index)))\n",
    "                    NN.global_nodes[node_index].num_incoming_connections += 1 # Each output starts off fully connected to input\n",
    "\n",
    "    def reset(self):\n",
    "        for i in self.nodes:\n",
    "            NN.global_nodes[i].val = 0\n",
    "            NN.global_nodes[i].received = 0\n",
    "\n",
    "    def clone(self, mutate=False, crossover=False):\n",
    "        # Create a new NN instance with same input/output dims\n",
    "        new_nn = NN(self.input_dim, self.output_dim, mutate, crossover)\n",
    "        \n",
    "        # Deep copy nodes\n",
    "        new_nn.nodes = []\n",
    "        for node in self.nodes:\n",
    "            new_node = Node(node.id, node.is_input, node.is_output)\n",
    "            new_node.val = node.val\n",
    "            new_node.num_incoming_connections = node.num_incoming_connections\n",
    "            new_node.received = node.received\n",
    "            new_nn.nodes.append(new_node)\n",
    "\n",
    "        # Deep copy connections\n",
    "        new_nn.connections = []\n",
    "        for conn in self.connections:\n",
    "            # Find the corresponding new nodes by id\n",
    "            in_node = next(n for n in new_nn.nodes if n.id == conn.in_node.id)\n",
    "            out_node = next(n for n in new_nn.nodes if n.id == conn.out_node.id)\n",
    "\n",
    "            new_conn = ConnectionGene(in_node, out_node, conn.weight, conn.innov_num)\n",
    "            new_conn.enable = conn.enable\n",
    "            new_nn.connections.append(new_conn)\n",
    "\n",
    "        return new_nn\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if x.shape[1] != self.input_dim:\n",
    "            raise ValueError(\"Input dim is not correct\")\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        self.reset()\n",
    "    \n",
    "        # Create batch versions of node values and received counts\n",
    "        for node in self.nodes:\n",
    "            node.vals = torch.zeros(batch_size)\n",
    "            node.received = torch.zeros(batch_size, dtype=torch.int)\n",
    "    \n",
    "        # Set input values\n",
    "        for idx in range(self.input_dim):\n",
    "            self.nodes[idx].vals = x[:, idx]\n",
    "    \n",
    "        # Start with nodes whose incoming connections are already satisfied\n",
    "        queue = deque()\n",
    "        for node in self.nodes:\n",
    "            if node.num_incoming_connections == 0:\n",
    "                queue.append(node)\n",
    "    \n",
    "        while queue:\n",
    "            curr_node = queue.popleft()\n",
    "    \n",
    "            for conn in self.connections:\n",
    "                if not conn.enable:\n",
    "                    continue\n",
    "                if conn.in_node != curr_node:\n",
    "                    continue\n",
    "    \n",
    "                out_node = conn.out_node\n",
    "                out_node.vals += curr_node.vals * conn.weight\n",
    "                out_node.received += 1\n",
    "    \n",
    "                # Only enqueue if all inputs are received\n",
    "                # Note: vectorized check â€” adds node to queue if all samples are ready\n",
    "                if (out_node.received == out_node.num_incoming_connections).all():\n",
    "                    if not out_node.is_output:\n",
    "                        out_node.vals = torch.relu(out_node.vals)\n",
    "                    queue.append(out_node)\n",
    "    \n",
    "        # Collect logits from output nodes\n",
    "        output_vals = [node.vals for node in self.nodes if node.is_output]\n",
    "        logits = torch.stack(output_vals, dim=1)  # shape: (batch_size, num_outputs)\n",
    "        return logits\n",
    "    \n",
    "    def weight_perturbation(self, quiet):\n",
    "        rand_conn_id = torch.randint(0, len(self.connections), (1,)).item()\n",
    "        \n",
    "        mean = 0.0\n",
    "        std_dev = 0.1\n",
    "        \n",
    "        noise = torch.randn_like(self.connections[rand_conn_id].weight) * std_dev + mean\n",
    "        self.connections[rand_conn_id].weight += noise\n",
    "        \n",
    "        if not quiet:\n",
    "            print(f'connection {rand_conn_id} weight perturbated to {self.connections[rand_conn_id].weight.item():.2f}')\n",
    "        \n",
    "    def weight_modification(self, quiet):\n",
    "        rand_conn_id = torch.randint(0, len(self.connections), (1,)).item()\n",
    "\n",
    "        self.connections[rand_conn_id].weight = torch.randn(1)\n",
    "        \n",
    "        if not quiet:\n",
    "            print(f'connection {rand_conn_id} weight modified to {self.connections[rand_conn_id].weight.item():.2f}')\n",
    "\n",
    "    def add_connection(self, quiet):\n",
    "        max_attempts = 100  # prevent infinite loop\n",
    "        for _ in range(max_attempts):\n",
    "            rand_node_in = self.nodes[torch.randint(0, len(self.nodes), (1,)).item()]\n",
    "        \n",
    "            if rand_node_in.is_output:\n",
    "                continue\n",
    "        \n",
    "            rand_node_out = self.nodes[torch.randint(0, len(self.nodes), (1,)).item()]\n",
    "        \n",
    "            if rand_node_out == rand_node_in or rand_node_out.is_input:\n",
    "                continue\n",
    "        \n",
    "            # Skip if connection already exists\n",
    "            if any(conn.in_node == rand_node_in and conn.out_node == rand_node_out for conn in self.connections):\n",
    "                continue\n",
    "        \n",
    "            # Optional: skip if this would form a cycle\n",
    "            # if self.creates_cycle(rand_node_in, rand_node_out):\n",
    "            #     continue\n",
    "        \n",
    "            conn = ConnectionGene(rand_node_in, rand_node_out, torch.randn(1), NN.glob_innov_num)\n",
    "            self.connections.append(conn)\n",
    "            NN.glob_innov_num += 1\n",
    "            rand_node_out.num_incoming_connections += 1\n",
    "            \n",
    "            if not quiet:\n",
    "                print(f\"Connection created from node {rand_node_in.id} to node {rand_node_out.id} (innovation #{NN.glob_innov_num})\")\n",
    "\n",
    "            return\n",
    "            \n",
    "        if not quiet:\n",
    "            print(\"Failed to add connection after max attempts.\")\n",
    "\n",
    "    def add_node(self, quiet):            \n",
    "        rand_conn_id = torch.randint(0, len(self.connections), (1,)).item()\n",
    "\n",
    "        while not self.connections[rand_conn_id].enable:\n",
    "            rand_conn_id = torch.randint(0, len(self.connections), (1,)).item()\n",
    "\n",
    "        # Splits an existing connection by adding a node\n",
    "        self.connections[rand_conn_id].enable = False\n",
    "        \n",
    "        new_node = Node(NN.glob_node_num)\n",
    "        self.nodes.append(new_node)\n",
    "        NN.glob_node_num += 1\n",
    "        \n",
    "        conn = ConnectionGene(self.connections[rand_conn_id].in_node, new_node, torch.randn(1), NN.glob_innov_num)\n",
    "        self.connections.append(conn)\n",
    "        NN.glob_innov_num += 1\n",
    "        \n",
    "        conn = ConnectionGene(new_node, self.connections[rand_conn_id].out_node, torch.randn(1), NN.glob_innov_num)\n",
    "        self.connections.append(conn)\n",
    "        NN.glob_innov_num += 1\n",
    "        \n",
    "        if not quiet:\n",
    "            print(f'connection {rand_conn_id} split')\n",
    "\n",
    "    def toggle_connection(self, quiet):\n",
    "        rand_conn_id = torch.randint(0, len(self.connections), (1,)).item()\n",
    "\n",
    "        self.connections[rand_conn_id].enable = not self.connections[rand_conn_id].enable\n",
    "\n",
    "        if not quiet:\n",
    "            print(f'connection {rand_conn_id} toggled to {self.connections[rand_conn_id].enable}')\n",
    "\n",
    "    def mutate(self, quiet=False):\n",
    "        # For this experiment i use 60 weight perturbation, 60 weight mutation, 5 add connection, 3 add node, 2 toggle\n",
    "        # Each one is chosen independently of each other\n",
    "        # Does not include crossover. That cannot be done by itself\n",
    "        new_model = self.clone(True)\n",
    "\n",
    "        # Do mutations on new_model\n",
    "        if random.random() < 0.60:\n",
    "            new_model.weight_perturbation(quiet)\n",
    "        if random.random() < 0.60:\n",
    "            new_model.weight_modification(quiet)\n",
    "        if random.random() < 0.05:\n",
    "            new_model.add_connection(quiet)\n",
    "        if random.random() < 0.03:\n",
    "            new_model.add_node(quiet)\n",
    "        if random.random() < 0.02:\n",
    "            new_model.toggle_connection(quiet)\n",
    "    \n",
    "        return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a08c669d-7c2d-4b73-993e-c2369ef3a35d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'while' statement on line 15 (1054849484.py, line 18)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn new_model\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'while' statement on line 15\n"
     ]
    }
   ],
   "source": [
    "def crossover(info1, info2):\n",
    "    # Equal Fitness\n",
    "    # Might not implement this rn because it doesnt happen that much\n",
    "    if info1['loss'] == info2['loss']:\n",
    "        pass\n",
    "\n",
    "    # Find fitter model\n",
    "    fit_model, less_fit_model = (info2['model'], info1['model']) if info1['loss'] > info2['loss'] else (info1['model'], info2['model'])\n",
    "\n",
    "    # New model starts off as clone of more fit\n",
    "    new_model = fit_model.clone(True)\n",
    "\n",
    "    fit_conn_pointer = less_fit_conn_pointer = 0\n",
    "\n",
    "    while fit_conn_pointer != len(fit_model.connections) - 1 and less_fit_conn_pointer != len(less_fit_model.connections) - 1:\n",
    "        # IDK\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f53ddb4b-c42d-4bb9-aab8-31764af7ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_compatibility(genome1, genome2, c1, c2, c3, delta_thresh):\n",
    "    genome1_conns = {i.innov_num: i.weight for i in genome1.connections}\n",
    "    genome2_conns = {i.innov_num: i.weight for i in genome2.connections}\n",
    "\n",
    "    innovs1 = set(genome1_conns.keys())\n",
    "    innovs2 = set(genome2_conns.keys())\n",
    "\n",
    "    max_innov1 = max(innovs1) if innovs1 else 0\n",
    "    max_innov2 = max(innovs2) if innovs2 else 0\n",
    "    max_innov = max(max_innov1, max_innov2)\n",
    "\n",
    "    # Matching genes: innovation numbers in both genomes\n",
    "    matching = innovs1.intersection(innovs2)\n",
    "    # Calculate average weight difference for matching genes\n",
    "    if matching:\n",
    "        weight_diff = sum(abs(genome1_conns[i] - genome2_conns[i]) for i in matching) / len(matching)\n",
    "    else:\n",
    "        weight_diff = 0\n",
    "\n",
    "    # Excess genes: genes whose innovation number is greater than max innovation number of other genome\n",
    "    excess = 0\n",
    "    for innov in innovs1:\n",
    "        if innov > max_innov2:\n",
    "            excess += 1\n",
    "    for innov in innovs2:\n",
    "        if innov > max_innov1:\n",
    "            excess += 1\n",
    "\n",
    "    # Disjoint genes: genes that do not match and are not excess\n",
    "    disjoint = (len(innovs1 - innovs2) + len(innovs2 - innovs1)) - excess\n",
    "\n",
    "    # Normalization factor N\n",
    "    N = max(len(genome1_conns), len(genome2_conns))\n",
    "    if N < 20:\n",
    "        N = 1  # as per original NEAT paper for small genomes\n",
    "\n",
    "    delta = (c1 * excess / N) + (c2 * disjoint / N) + (c3 * weight_diff)\n",
    "    \n",
    "    # return delta\n",
    "    return True if delta < delta_thresh else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fa873000-3f48-41af-a8d7-11832bb03591",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_infos = [{\"model\": NN(64, 10), \"loss\": float('inf')} for _ in range(10)]\n",
    "# top_models = [info[\"model\"] for info in model_infos[:5]]\n",
    "\n",
    "# for model in top_models:\n",
    "#         model_infos.append({\"model\": model.mutate(), \"loss\": float('inf')})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "27876d56-916c-4695-b0b7-9cb94b94bd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "top model loss: 49.73\n",
      "epoch: 1\n",
      "top model loss: 49.39\n",
      "epoch: 2\n",
      "top model loss: 46.70\n",
      "epoch: 3\n",
      "top model loss: 44.24\n",
      "epoch: 4\n",
      "top model loss: 44.56\n",
      "epoch: 5\n",
      "top model loss: 42.57\n",
      "epoch: 6\n",
      "top model loss: 41.27\n",
      "epoch: 7\n",
      "top model loss: 40.95\n",
      "epoch: 8\n",
      "top model loss: 40.61\n",
      "epoch: 9\n",
      "top model loss: 40.59\n",
      "epoch: 10\n",
      "top model loss: 38.80\n",
      "epoch: 11\n",
      "top model loss: 36.54\n",
      "epoch: 12\n",
      "top model loss: 36.41\n",
      "epoch: 13\n",
      "top model loss: 36.33\n",
      "epoch: 14\n",
      "top model loss: 35.70\n",
      "epoch: 15\n",
      "top model loss: 35.41\n",
      "epoch: 16\n",
      "top model loss: 35.19\n",
      "epoch: 17\n",
      "top model loss: 35.17\n",
      "epoch: 18\n",
      "top model loss: 35.00\n",
      "epoch: 19\n",
      "top model loss: 34.77\n",
      "epoch: 20\n",
      "top model loss: 33.76\n",
      "epoch: 21\n",
      "top model loss: 33.78\n",
      "epoch: 22\n",
      "top model loss: 33.15\n",
      "epoch: 23\n",
      "top model loss: 32.50\n",
      "epoch: 24\n",
      "top model loss: 31.95\n",
      "epoch: 25\n",
      "top model loss: 31.61\n",
      "epoch: 26\n",
      "top model loss: 31.53\n",
      "epoch: 27\n",
      "top model loss: 31.19\n",
      "epoch: 28\n",
      "top model loss: 30.15\n",
      "epoch: 29\n",
      "top model loss: 30.02\n",
      "epoch: 30\n",
      "top model loss: 30.30\n",
      "epoch: 31\n",
      "top model loss: 30.44\n",
      "epoch: 32\n",
      "top model loss: 29.62\n",
      "epoch: 33\n",
      "top model loss: 29.78\n",
      "epoch: 34\n",
      "top model loss: 29.12\n",
      "epoch: 35\n",
      "top model loss: 28.68\n",
      "epoch: 36\n",
      "top model loss: 27.66\n",
      "epoch: 37\n",
      "top model loss: 28.05\n",
      "epoch: 38\n",
      "top model loss: 27.78\n",
      "epoch: 39\n",
      "top model loss: 27.75\n",
      "epoch: 40\n",
      "top model loss: 26.58\n",
      "epoch: 41\n",
      "top model loss: 25.96\n",
      "epoch: 42\n",
      "top model loss: 25.21\n",
      "epoch: 43\n",
      "top model loss: 24.25\n",
      "epoch: 44\n",
      "top model loss: 23.83\n",
      "epoch: 45\n",
      "top model loss: 24.01\n",
      "epoch: 46\n",
      "top model loss: 23.26\n",
      "epoch: 47\n",
      "top model loss: 23.57\n",
      "epoch: 48\n",
      "top model loss: 23.18\n",
      "epoch: 49\n",
      "top model loss: 23.09\n",
      "epoch: 50\n",
      "top model loss: 22.89\n",
      "epoch: 51\n",
      "top model loss: 22.81\n",
      "epoch: 52\n",
      "top model loss: 22.23\n",
      "epoch: 53\n",
      "top model loss: 21.86\n",
      "epoch: 54\n",
      "top model loss: 21.85\n",
      "epoch: 55\n",
      "top model loss: 21.63\n",
      "epoch: 56\n",
      "top model loss: 21.41\n",
      "epoch: 57\n",
      "top model loss: 21.15\n",
      "epoch: 58\n",
      "top model loss: 21.20\n",
      "epoch: 59\n",
      "top model loss: 20.79\n",
      "epoch: 60\n",
      "top model loss: 20.76\n",
      "epoch: 61\n",
      "top model loss: 20.69\n",
      "epoch: 62\n",
      "top model loss: 19.62\n",
      "epoch: 63\n",
      "top model loss: 19.26\n",
      "epoch: 64\n",
      "top model loss: 19.36\n",
      "epoch: 65\n",
      "top model loss: 18.82\n",
      "epoch: 66\n",
      "top model loss: 17.43\n",
      "epoch: 67\n",
      "top model loss: 16.93\n",
      "epoch: 68\n",
      "top model loss: 17.02\n",
      "epoch: 69\n",
      "top model loss: 16.81\n",
      "epoch: 70\n",
      "top model loss: 16.61\n",
      "epoch: 71\n",
      "top model loss: 16.21\n",
      "epoch: 72\n",
      "top model loss: 16.48\n",
      "epoch: 73\n",
      "top model loss: 16.36\n",
      "epoch: 74\n",
      "top model loss: 16.51\n",
      "epoch: 75\n",
      "top model loss: 15.87\n",
      "epoch: 76\n",
      "top model loss: 15.67\n",
      "epoch: 77\n",
      "top model loss: 15.36\n",
      "epoch: 78\n",
      "top model loss: 15.48\n",
      "epoch: 79\n",
      "top model loss: 15.04\n",
      "epoch: 80\n",
      "top model loss: 14.96\n",
      "epoch: 81\n",
      "top model loss: 14.56\n",
      "epoch: 82\n",
      "top model loss: 14.36\n",
      "epoch: 83\n",
      "top model loss: 14.46\n",
      "epoch: 84\n",
      "top model loss: 14.37\n",
      "epoch: 85\n",
      "top model loss: 14.55\n",
      "epoch: 86\n",
      "top model loss: 14.18\n",
      "epoch: 87\n",
      "top model loss: 13.69\n",
      "epoch: 88\n",
      "top model loss: 14.05\n",
      "epoch: 89\n",
      "top model loss: 13.35\n",
      "epoch: 90\n",
      "top model loss: 13.39\n",
      "epoch: 91\n",
      "top model loss: 12.96\n",
      "epoch: 92\n",
      "top model loss: 12.85\n",
      "epoch: 93\n",
      "top model loss: 12.85\n",
      "epoch: 94\n",
      "top model loss: 12.17\n",
      "epoch: 95\n",
      "top model loss: 12.20\n",
      "epoch: 96\n",
      "top model loss: 12.27\n",
      "epoch: 97\n",
      "top model loss: 12.24\n",
      "epoch: 98\n",
      "top model loss: 11.92\n",
      "epoch: 99\n",
      "top model loss: 11.93\n",
      "epoch: 100\n",
      "top model loss: 11.65\n",
      "epoch: 101\n",
      "top model loss: 11.68\n",
      "epoch: 102\n",
      "top model loss: 11.74\n",
      "epoch: 103\n",
      "top model loss: 11.53\n",
      "epoch: 104\n",
      "top model loss: 11.22\n",
      "epoch: 105\n",
      "top model loss: 10.96\n",
      "epoch: 106\n",
      "top model loss: 10.76\n",
      "epoch: 107\n",
      "top model loss: 10.74\n",
      "epoch: 108\n",
      "top model loss: 10.62\n",
      "epoch: 109\n",
      "top model loss: 10.45\n",
      "epoch: 110\n",
      "top model loss: 10.54\n",
      "epoch: 111\n",
      "top model loss: 10.30\n",
      "epoch: 112\n",
      "top model loss: 10.25\n",
      "epoch: 113\n",
      "top model loss: 10.23\n",
      "epoch: 114\n",
      "top model loss: 10.59\n",
      "epoch: 115\n",
      "top model loss: 10.23\n",
      "epoch: 116\n",
      "top model loss: 10.24\n",
      "epoch: 117\n",
      "top model loss: 10.15\n",
      "epoch: 118\n",
      "top model loss: 10.14\n",
      "epoch: 119\n",
      "top model loss: 10.03\n",
      "epoch: 120\n",
      "top model loss: 10.10\n",
      "epoch: 121\n",
      "top model loss: 9.97\n",
      "epoch: 122\n",
      "top model loss: 10.08\n",
      "epoch: 123\n",
      "top model loss: 9.61\n",
      "epoch: 124\n",
      "top model loss: 9.56\n",
      "epoch: 125\n",
      "top model loss: 9.57\n",
      "epoch: 126\n",
      "top model loss: 9.61\n",
      "epoch: 127\n",
      "top model loss: 9.40\n",
      "epoch: 128\n",
      "top model loss: 9.66\n",
      "epoch: 129\n",
      "top model loss: 9.60\n",
      "epoch: 130\n",
      "top model loss: 9.64\n",
      "epoch: 131\n",
      "top model loss: 9.68\n",
      "epoch: 132\n",
      "top model loss: 9.72\n",
      "epoch: 133\n",
      "top model loss: 9.36\n",
      "epoch: 134\n",
      "top model loss: 9.44\n",
      "epoch: 135\n",
      "top model loss: 9.58\n",
      "epoch: 136\n",
      "top model loss: 9.61\n",
      "epoch: 137\n",
      "top model loss: 9.53\n",
      "epoch: 138\n",
      "top model loss: 9.44\n",
      "epoch: 139\n",
      "top model loss: 9.10\n",
      "epoch: 140\n",
      "top model loss: 9.12\n",
      "epoch: 141\n",
      "top model loss: 8.95\n",
      "epoch: 142\n",
      "top model loss: 8.89\n",
      "epoch: 143\n",
      "top model loss: 9.05\n",
      "epoch: 144\n",
      "top model loss: 8.78\n",
      "epoch: 145\n",
      "top model loss: 8.64\n",
      "epoch: 146\n",
      "top model loss: 8.48\n",
      "epoch: 147\n",
      "top model loss: 8.45\n",
      "epoch: 148\n",
      "top model loss: 8.43\n",
      "epoch: 149\n",
      "top model loss: 8.32\n",
      "epoch: 150\n",
      "top model loss: 8.21\n",
      "epoch: 151\n",
      "top model loss: 8.05\n",
      "epoch: 152\n",
      "top model loss: 7.95\n",
      "epoch: 153\n",
      "top model loss: 7.72\n",
      "epoch: 154\n",
      "top model loss: 7.73\n",
      "epoch: 155\n",
      "top model loss: 7.68\n",
      "epoch: 156\n",
      "top model loss: 7.72\n",
      "epoch: 157\n",
      "top model loss: 7.60\n",
      "epoch: 158\n",
      "top model loss: 7.70\n",
      "epoch: 159\n",
      "top model loss: 7.67\n",
      "epoch: 160\n",
      "top model loss: 7.69\n",
      "epoch: 161\n",
      "top model loss: 7.51\n",
      "epoch: 162\n",
      "top model loss: 7.33\n",
      "epoch: 163\n",
      "top model loss: 7.31\n",
      "epoch: 164\n",
      "top model loss: 7.34\n",
      "epoch: 165\n",
      "top model loss: 7.33\n",
      "epoch: 166\n",
      "top model loss: 7.50\n",
      "epoch: 167\n",
      "top model loss: 7.39\n",
      "epoch: 168\n",
      "top model loss: 7.12\n",
      "epoch: 169\n",
      "top model loss: 6.95\n",
      "epoch: 170\n",
      "top model loss: 6.95\n",
      "epoch: 171\n",
      "top model loss: 6.87\n",
      "epoch: 172\n",
      "top model loss: 6.68\n",
      "epoch: 173\n",
      "top model loss: 6.59\n",
      "epoch: 174\n",
      "top model loss: 6.63\n",
      "epoch: 175\n",
      "top model loss: 6.62\n",
      "epoch: 176\n",
      "top model loss: 6.54\n",
      "epoch: 177\n",
      "top model loss: 6.37\n",
      "epoch: 178\n",
      "top model loss: 6.39\n",
      "epoch: 179\n",
      "top model loss: 6.42\n",
      "epoch: 180\n",
      "top model loss: 6.39\n",
      "epoch: 181\n",
      "top model loss: 6.37\n",
      "epoch: 182\n",
      "top model loss: 6.36\n",
      "epoch: 183\n",
      "top model loss: 6.38\n",
      "epoch: 184\n",
      "top model loss: 6.42\n",
      "epoch: 185\n",
      "top model loss: 6.20\n",
      "epoch: 186\n",
      "top model loss: 6.35\n",
      "epoch: 187\n",
      "top model loss: 6.31\n",
      "epoch: 188\n",
      "top model loss: 6.28\n",
      "epoch: 189\n",
      "top model loss: 6.36\n",
      "epoch: 190\n",
      "top model loss: 6.33\n",
      "epoch: 191\n",
      "top model loss: 6.25\n",
      "epoch: 192\n",
      "top model loss: 6.15\n",
      "epoch: 193\n",
      "top model loss: 5.91\n",
      "epoch: 194\n",
      "top model loss: 5.96\n",
      "epoch: 195\n",
      "top model loss: 5.97\n",
      "epoch: 196\n",
      "top model loss: 6.00\n",
      "epoch: 197\n",
      "top model loss: 5.92\n",
      "epoch: 198\n",
      "top model loss: 5.95\n",
      "epoch: 199\n",
      "top model loss: 5.83\n",
      "epoch: 200\n",
      "top model loss: 5.71\n",
      "epoch: 201\n",
      "top model loss: 5.69\n",
      "epoch: 202\n",
      "top model loss: 5.80\n",
      "epoch: 203\n",
      "top model loss: 5.73\n",
      "epoch: 204\n",
      "top model loss: 5.81\n",
      "epoch: 205\n",
      "top model loss: 5.81\n",
      "epoch: 206\n",
      "top model loss: 5.75\n",
      "epoch: 207\n",
      "top model loss: 5.78\n",
      "epoch: 208\n",
      "top model loss: 5.62\n",
      "epoch: 209\n",
      "top model loss: 5.59\n",
      "epoch: 210\n",
      "top model loss: 5.67\n",
      "epoch: 211\n",
      "top model loss: 5.66\n",
      "epoch: 212\n",
      "top model loss: 5.72\n",
      "epoch: 213\n",
      "top model loss: 5.48\n",
      "epoch: 214\n",
      "top model loss: 5.49\n",
      "epoch: 215\n",
      "top model loss: 5.51\n",
      "epoch: 216\n",
      "top model loss: 5.37\n",
      "epoch: 217\n",
      "top model loss: 5.32\n",
      "epoch: 218\n",
      "top model loss: 5.42\n",
      "epoch: 219\n",
      "top model loss: 5.35\n",
      "epoch: 220\n",
      "top model loss: 5.50\n",
      "epoch: 221\n",
      "top model loss: 5.23\n",
      "epoch: 222\n",
      "top model loss: 5.18\n",
      "epoch: 223\n",
      "top model loss: 5.31\n",
      "epoch: 224\n",
      "top model loss: 5.27\n",
      "epoch: 225\n",
      "top model loss: 5.16\n",
      "epoch: 226\n",
      "top model loss: 5.06\n",
      "epoch: 227\n",
      "top model loss: 5.18\n",
      "epoch: 228\n",
      "top model loss: 5.14\n",
      "epoch: 229\n",
      "top model loss: 5.02\n",
      "epoch: 230\n",
      "top model loss: 4.92\n",
      "epoch: 231\n",
      "top model loss: 4.83\n",
      "epoch: 232\n",
      "top model loss: 4.89\n",
      "epoch: 233\n",
      "top model loss: 4.84\n",
      "epoch: 234\n",
      "top model loss: 4.81\n",
      "epoch: 235\n",
      "top model loss: 4.85\n",
      "epoch: 236\n",
      "top model loss: 4.94\n",
      "epoch: 237\n",
      "top model loss: 4.84\n",
      "epoch: 238\n",
      "top model loss: 4.93\n",
      "epoch: 239\n",
      "top model loss: 4.90\n",
      "epoch: 240\n",
      "top model loss: 4.88\n",
      "epoch: 241\n",
      "top model loss: 4.88\n",
      "epoch: 242\n",
      "top model loss: 4.90\n",
      "epoch: 243\n",
      "top model loss: 4.72\n",
      "epoch: 244\n",
      "top model loss: 4.82\n",
      "epoch: 245\n",
      "top model loss: 4.73\n",
      "epoch: 246\n",
      "top model loss: 4.82\n",
      "epoch: 247\n",
      "top model loss: 4.79\n",
      "epoch: 248\n",
      "top model loss: 4.75\n",
      "epoch: 249\n",
      "top model loss: 4.79\n",
      "epoch: 250\n",
      "top model loss: 4.62\n",
      "epoch: 251\n",
      "top model loss: 4.63\n",
      "epoch: 252\n",
      "top model loss: 4.56\n",
      "epoch: 253\n",
      "top model loss: 4.66\n",
      "epoch: 254\n",
      "top model loss: 4.52\n",
      "epoch: 255\n",
      "top model loss: 4.49\n",
      "epoch: 256\n",
      "top model loss: 4.46\n",
      "epoch: 257\n",
      "top model loss: 4.49\n",
      "epoch: 258\n",
      "top model loss: 4.58\n",
      "epoch: 259\n",
      "top model loss: 4.66\n",
      "epoch: 260\n",
      "top model loss: 4.62\n",
      "epoch: 261\n",
      "top model loss: 4.61\n",
      "epoch: 262\n",
      "top model loss: 4.52\n",
      "epoch: 263\n",
      "top model loss: 4.58\n",
      "epoch: 264\n",
      "top model loss: 4.56\n",
      "epoch: 265\n",
      "top model loss: 4.47\n",
      "epoch: 266\n",
      "top model loss: 4.52\n",
      "epoch: 267\n",
      "top model loss: 4.50\n",
      "epoch: 268\n",
      "top model loss: 4.53\n",
      "epoch: 269\n",
      "top model loss: 4.57\n",
      "epoch: 270\n",
      "top model loss: 4.62\n",
      "epoch: 271\n",
      "top model loss: 4.65\n",
      "epoch: 272\n",
      "top model loss: 4.58\n",
      "epoch: 273\n",
      "top model loss: 4.59\n",
      "epoch: 274\n",
      "top model loss: 4.56\n",
      "epoch: 275\n",
      "top model loss: 4.57\n",
      "epoch: 276\n",
      "top model loss: 4.38\n",
      "epoch: 277\n",
      "top model loss: 4.35\n",
      "epoch: 278\n",
      "top model loss: 4.32\n",
      "epoch: 279\n",
      "top model loss: 4.34\n",
      "epoch: 280\n",
      "top model loss: 4.29\n",
      "epoch: 281\n",
      "top model loss: 4.22\n",
      "epoch: 282\n",
      "top model loss: 4.24\n",
      "epoch: 283\n",
      "top model loss: 4.23\n",
      "epoch: 284\n",
      "top model loss: 4.13\n",
      "epoch: 285\n",
      "top model loss: 4.20\n",
      "epoch: 286\n",
      "top model loss: 4.19\n",
      "epoch: 287\n",
      "top model loss: 4.24\n",
      "epoch: 288\n",
      "top model loss: 4.24\n",
      "epoch: 289\n",
      "top model loss: 4.15\n",
      "epoch: 290\n",
      "top model loss: 4.07\n",
      "epoch: 291\n",
      "top model loss: 4.11\n",
      "epoch: 292\n",
      "top model loss: 4.08\n",
      "epoch: 293\n",
      "top model loss: 4.12\n",
      "epoch: 294\n",
      "top model loss: 4.07\n",
      "epoch: 295\n",
      "top model loss: 4.10\n",
      "epoch: 296\n",
      "top model loss: 4.08\n",
      "epoch: 297\n",
      "top model loss: 3.95\n",
      "epoch: 298\n",
      "top model loss: 4.06\n",
      "epoch: 299\n",
      "top model loss: 4.04\n",
      "epoch: 300\n",
      "top model loss: 3.99\n",
      "epoch: 301\n",
      "top model loss: 4.04\n",
      "epoch: 302\n",
      "top model loss: 3.97\n",
      "epoch: 303\n",
      "top model loss: 3.82\n",
      "epoch: 304\n",
      "top model loss: 3.73\n",
      "epoch: 305\n",
      "top model loss: 3.78\n",
      "epoch: 306\n",
      "top model loss: 3.75\n",
      "epoch: 307\n",
      "top model loss: 3.75\n",
      "epoch: 308\n",
      "top model loss: 3.85\n",
      "epoch: 309\n",
      "top model loss: 3.81\n",
      "epoch: 310\n",
      "top model loss: 3.73\n",
      "epoch: 311\n",
      "top model loss: 3.72\n",
      "epoch: 312\n",
      "top model loss: 3.67\n",
      "epoch: 313\n",
      "top model loss: 3.69\n",
      "epoch: 314\n",
      "top model loss: 3.71\n",
      "epoch: 315\n",
      "top model loss: 3.67\n",
      "epoch: 316\n",
      "top model loss: 3.63\n",
      "epoch: 317\n",
      "top model loss: 3.61\n",
      "epoch: 318\n",
      "top model loss: 3.59\n",
      "epoch: 319\n",
      "top model loss: 3.57\n",
      "epoch: 320\n",
      "top model loss: 3.58\n",
      "epoch: 321\n",
      "top model loss: 3.58\n",
      "epoch: 322\n",
      "top model loss: 3.61\n",
      "epoch: 323\n",
      "top model loss: 3.57\n",
      "epoch: 324\n",
      "top model loss: 3.58\n",
      "epoch: 325\n",
      "top model loss: 3.64\n",
      "epoch: 326\n",
      "top model loss: 3.57\n",
      "epoch: 327\n",
      "top model loss: 3.54\n",
      "epoch: 328\n",
      "top model loss: 3.66\n",
      "epoch: 329\n",
      "top model loss: 3.73\n",
      "epoch: 330\n",
      "top model loss: 3.63\n",
      "epoch: 331\n",
      "top model loss: 3.42\n",
      "epoch: 332\n",
      "top model loss: 3.39\n",
      "epoch: 333\n",
      "top model loss: 3.30\n",
      "epoch: 334\n",
      "top model loss: 3.33\n",
      "epoch: 335\n",
      "top model loss: 3.40\n",
      "epoch: 336\n",
      "top model loss: 3.37\n",
      "epoch: 337\n",
      "top model loss: 3.36\n",
      "epoch: 338\n",
      "top model loss: 3.39\n",
      "epoch: 339\n",
      "top model loss: 3.42\n",
      "epoch: 340\n",
      "top model loss: 3.31\n",
      "epoch: 341\n",
      "top model loss: 3.38\n",
      "epoch: 342\n",
      "top model loss: 3.35\n",
      "epoch: 343\n",
      "top model loss: 3.22\n",
      "epoch: 344\n",
      "top model loss: 3.19\n",
      "epoch: 345\n",
      "top model loss: 3.15\n",
      "epoch: 346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x103da63c0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/paullin/Documents/neuroevol/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top model loss: 3.19\n",
      "epoch: 347\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m total_samples = \u001b[32m0\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data_batch, label_batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     loss = loss_fn(output, label_batch)\n\u001b[32m     54\u001b[39m     total_loss += loss.item() * data_batch.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neuroevol/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/neuroevol/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 155\u001b[39m, in \u001b[36mNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    151\u001b[39m out_node.received += \u001b[32m1\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# Only enqueue if all inputs are received\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# Note: vectorized check â€” adds node to queue if all samples are ready\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mout_node\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreceived\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_node\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_incoming_connections\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_node.is_output:\n\u001b[32m    157\u001b[39m         out_node.vals = torch.relu(out_node.vals)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# \"Training loop\"\n",
    "import math\n",
    "\n",
    "# Hyperparameters\n",
    "population_size = 100\n",
    "epochs = 500\n",
    "input_dim = 64\n",
    "output_dim = 10\n",
    "top_k = 0.2 # The percentage of genomes to keep for reproduction\n",
    "\n",
    "# hyperparameters for measuring compatibility from https://nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf\n",
    "c1 = 1.0\n",
    "c2 = 1.0\n",
    "c3 = 3.0\n",
    "delta_thresh = 4.0\n",
    "\n",
    "# Init\n",
    "# Using list of lists\n",
    "# Dead species will not be kept track of. There will be no empty list\n",
    "population = []\n",
    "\n",
    "for _ in range(population_size):\n",
    "    new_model = {\"model\": NN(input_dim, output_dim), \"loss\": float('inf'), \"fitness\": -float('inf')}\n",
    "\n",
    "    # First model\n",
    "    if len(population) == 0:\n",
    "        population.append([new_model])\n",
    "    else:\n",
    "        for idx, species in enumerate(population):\n",
    "            if measure_compatibility(new_model['model'], species[0]['model'], c1, c2, c3, delta_thresh):\n",
    "                population[idx].append(new_model)\n",
    "            else:\n",
    "                # New species created\n",
    "                population.append([new_model])\n",
    "                \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    \n",
    "    for species in population:\n",
    "        for model_info in species:\n",
    "            model_info[\"model\"].reset()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for model_info in species:\n",
    "                model = model_info[\"model\"]\n",
    "                total_loss = 0.0\n",
    "                total_samples = 0\n",
    "    \n",
    "                for data_batch, label_batch in loader:\n",
    "                    output = model(data_batch)\n",
    "                    loss = loss_fn(output, label_batch)\n",
    "                    total_loss += loss.item() * data_batch.size(0)\n",
    "                    total_samples += data_batch.size(0)\n",
    "                \n",
    "                model_info[\"loss\"] = total_loss / total_samples\n",
    "\n",
    "    flattened_population = []\n",
    "\n",
    "    for species in population:\n",
    "        for genome in species:\n",
    "            flattened_population.append(genome)\n",
    "            \n",
    "    ranked_models = sorted([model_info for model_info in flattened_population], key=lambda x: x[\"loss\"])\n",
    "    print(f\"top model loss: {ranked_models[0]['loss']:.2f}\")\n",
    "\n",
    "    # Fitness sharing\n",
    "    for species in population:\n",
    "        species_size = len(species)\n",
    "        for genome in species:\n",
    "            raw_fitness = 1 / (1 + genome['loss'])\n",
    "            genome['fitness'] = raw_fitness / species_size\n",
    "\n",
    "    # This is just a list not a list of lists\n",
    "    new_population = []\n",
    "\n",
    "    for species in population:\n",
    "        offspring = []\n",
    "        \n",
    "        ranked_models = sorted([model_info for model_info in species], key=lambda x: x[\"fitness\"], reverse=True)\n",
    "        parents = [model_info['model'] for model_info in ranked_models[:math.ceil(top_k * len(ranked_models))]]\n",
    "\n",
    "        while len(offspring) != len(ranked_models):\n",
    "            offspring.append({\"model\": random.choice(parents).mutate(True), \"loss\": float('inf'), \"fitness\": -float('inf')})\n",
    "            \n",
    "        new_population.extend(offspring)\n",
    "\n",
    "    # Redivide into species\n",
    "    new_population_divided = []\n",
    "\n",
    "    for model in new_population:    \n",
    "        # First model\n",
    "        if len(new_population_divided) == 0:\n",
    "            new_population_divided.append([model])\n",
    "        else:\n",
    "            for idx, species in enumerate(new_population_divided):\n",
    "                if measure_compatibility(model['model'], species[0]['model'], c1, c2, c3, delta_thresh):\n",
    "                    new_population_divided[idx].append(model)\n",
    "                else:\n",
    "                    # New species created\n",
    "                    new_population_divided.append([model])\n",
    "                    \n",
    "    population = new_population_divided\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a3d1afc7-bc50-4463-a2e9-1dd53fa81dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(64, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5454f1fd-ee56-4bdc-81c8-b1c5b2fd6566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -8.2188, -15.4980, -83.0724,  33.3792, -99.2694,   3.3089,  24.0452,\n",
       "        -37.0231, -81.9885,  47.4248])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fa66cc0e-53b7-49d5-ab65-9d64d27e09d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection 597 weight perturbated to 1.89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NN()"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.mutate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4e3e0-9bbb-4024-a506-dd98805d749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ca278-7301-4bbc-b224-0591355faeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(data_tensor[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroevol",
   "language": "python",
   "name": "neuroevol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
