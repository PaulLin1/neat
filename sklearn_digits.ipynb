{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "435996a1-f11a-4189-91fa-b227beb18c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "print(digits.data.shape)\n",
    "print(digits.target[44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "01c32047-eeed-4b98-b75a-4718a09fa911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9ef0358e-d2fc-4268-bb94-603ed55c71e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.Tensor(digits.data)\n",
    "target_tensor = F.one_hot(torch.Tensor(digits.target).long(), 10).float()\n",
    "\n",
    "dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "# Create dataloader\n",
    "# Wait we are not doing backpropogation so cant we just make batch_size the entire thing?\n",
    "batch_size = 1797\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765f546-52dd-41fb-ba34-567b2257cbad",
   "metadata": {},
   "source": [
    "NEAT PYTORCH\n",
    "\n",
    "to do\n",
    "genome species population stuff\n",
    "crossover\n",
    "improve parallelization i think\n",
    "\n",
    "Parallelize the eval of each nn. The nn is not parallelizable because mm cannot be used. \n",
    "\n",
    "\n",
    "rn every new connection gets an innovation number. this is wrong. you should check if they exist. if they exists you have to copy that innov number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1176c120-0522-440f-945d-0848012d838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEAT Classes\n",
    "\n",
    "from collections import deque, defaultdict\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, id_, input_=False, output=False):\n",
    "        self.id = id_\n",
    "        self.is_input = input_\n",
    "        self.is_output = output\n",
    "\n",
    "        self.val = torch.Tensor(0) # Tensors\n",
    "        self.num_incoming_connections = 0\n",
    "        \n",
    "        if not input_ and not output:\n",
    "            # Output has special handling when init because it starts off fully connected\n",
    "            # These are nodes that are created from splitting connection\n",
    "            self.num_incoming_connections = 1\n",
    "            \n",
    "        self.received = 0 # Keep track of nodes received before applying activation function\n",
    "        \n",
    "class ConnectionGene:\n",
    "    def __init__(self, in_node, out_node, innov_num, weight):\n",
    "        self.in_node = in_node # Nodes not node id\n",
    "        self.out_node = out_node\n",
    "        self.innov_num = innov_num\n",
    "        \n",
    "        self.weight = weight # Weights are tensors\n",
    "        self.enable = True # If node is disabled, it CAN be reenabled\n",
    "\n",
    "class NN(nn.Module):\n",
    "    # All nodes. so we can number new nodes. Might be able to make this an int to keep track of how many nodes\n",
    "    nodes = set()\n",
    "    # item is (in, out) index is the innov_num out is resulting node\n",
    "    global_connections = {}\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, mutate=False, crossover=False):\n",
    "        super(NN, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.nodes = [] # Nodes objects in this specific NN\n",
    "        self.connections = [] # Connections objects in this specific NN\n",
    "\n",
    "        # Mutated models should not be inited!\n",
    "        if mutate:\n",
    "            return\n",
    "        elif crossover:\n",
    "            # Initialized in a different way\n",
    "            self.input_dim = input_dim\n",
    "            self.output_dim = output_dim\n",
    "            \n",
    "            self.nodes = []\n",
    "            self.connections = []\n",
    "            \n",
    "        else:\n",
    "            # When init a new model, the node nums and innov should be the same as any other new initialized model\n",
    "            # This else only gets models in the initial population\n",
    "            # Initalize a fully connected NN with no hidden layers\n",
    "            for i in range(input_dim):\n",
    "                if i not in NN.nodes:\n",
    "                    NN.nodes.add(i)\n",
    "                self.nodes.append(Node(i, True))\n",
    "    \n",
    "            for i in range(output_dim):\n",
    "                node_index = i + input_dim\n",
    "                if node_index not in NN.nodes:\n",
    "                    NN.nodes.add(node_index)\n",
    "                self.nodes.append(Node(node_index, False, True))\n",
    "\n",
    "                for in_id in range(input_dim):\n",
    "                    in_out_tuple = (in_id, node_index)\n",
    "                    if in_out_tuple not in list(NN.global_connections.keys()):\n",
    "                        NN.global_connections.update({in_out_tuple: None}) # No resulting node until it is split for the first time\n",
    "                    innov_num = list(NN.global_connections.keys()).index(in_out_tuple)\n",
    "\n",
    "                    self.connections.append(ConnectionGene(self.nodes[in_id], self.nodes[node_index], innov_num, torch.randn(1)))\n",
    "                    self.nodes[node_index].num_incoming_connections += 1 # Each output starts off fully connected to input\n",
    "\n",
    "    def clone(self, mutate=False, crossover=False):\n",
    "        # Create a new NN instance with same input/output dims\n",
    "        new_nn = NN(self.input_dim, self.output_dim, mutate, crossover)\n",
    "        \n",
    "        # Deep copy nodes\n",
    "        new_nn.nodes = []\n",
    "        for node in self.nodes:\n",
    "            new_node = Node(node.id, node.is_input, node.is_output)\n",
    "            new_node.num_incoming_connections = node.num_incoming_connections\n",
    "            new_nn.nodes.append(new_node)\n",
    "\n",
    "        # Deep copy connections\n",
    "        new_nn.connections = []\n",
    "        for conn in self.connections:\n",
    "            # Find the corresponding new nodes by id\n",
    "            in_node = next(n for n in new_nn.nodes if n.id == conn.in_node.id)\n",
    "            out_node = next(n for n in new_nn.nodes if n.id == conn.out_node.id)\n",
    "\n",
    "            new_conn = ConnectionGene(in_node, out_node, conn.innov_num, conn.weight.clone().detach())\n",
    "            new_conn.enable = conn.enable\n",
    "            new_nn.connections.append(new_conn)\n",
    "\n",
    "        return new_nn\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if x.shape[1] != self.input_dim:\n",
    "            raise ValueError(\"Input dim is not correct\")\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "    \n",
    "        # Create batch versions of node values and received counts and reset them to zero\n",
    "        for node in self.nodes:\n",
    "            node.val = torch.zeros(batch_size)\n",
    "            node.received = torch.zeros(batch_size, dtype=torch.int)\n",
    "    \n",
    "        # Set input values\n",
    "        for idx in range(self.input_dim):\n",
    "            self.nodes[idx].val = x[:, idx]\n",
    "\n",
    "        # Start with nodes whose incoming connections are already satisfied AKA input nodes\n",
    "        queue = deque()\n",
    "        for node in self.nodes:\n",
    "            if node.num_incoming_connections == 0:\n",
    "                queue.append(node)\n",
    "    \n",
    "        while queue:\n",
    "            curr_node = queue.popleft()\n",
    "    \n",
    "            for conn in self.connections:\n",
    "                if not conn.enable:\n",
    "                    continue\n",
    "                if conn.in_node != curr_node:\n",
    "                    continue\n",
    "\n",
    "                out_node = conn.out_node\n",
    "                out_node.val += (curr_node.val * conn.weight)\n",
    "\n",
    "                out_node.received += 1\n",
    "    \n",
    "                # Only enqueue if all inputs are received\n",
    "                # Note: vectorized check â€” adds node to queue if all samples are ready\n",
    "                if (out_node.received == out_node.num_incoming_connections).all():\n",
    "                    if not out_node.is_output:\n",
    "                        out_node.val = torch.relu(out_node.val)\n",
    "                    queue.append(out_node)\n",
    "    \n",
    "        # Collect logits from output nodes\n",
    "        output_vals = [node.val for node in self.nodes if node.is_output]\n",
    "        logits = torch.stack(output_vals, dim=1)  # shape: (batch_size, num_outputs)\n",
    "        return logits\n",
    "\n",
    "    # I think in the paper perturbation and modification are the same but i did different\n",
    "    def weight_perturbation(self, quiet):\n",
    "        rand_conn_id = torch.randint(0, len(self.connections), (1,)).item()\n",
    "        \n",
    "        mean = 0.0\n",
    "        std_dev = 0.1\n",
    "        \n",
    "        noise = torch.randn_like(self.connections[rand_conn_id].weight) * std_dev + mean\n",
    "        self.connections[rand_conn_id].weight += noise\n",
    "        \n",
    "        if not quiet:\n",
    "            print(f'connection {rand_conn_id} weight perturbated to {self.connections[rand_conn_id].weight.item():.2f}')\n",
    "        \n",
    "    def weight_modification(self, quiet):\n",
    "        rand_conn_id = torch.randint(0, len(self.connections), (1,)).item()\n",
    "\n",
    "        self.connections[rand_conn_id].weight = torch.randn(1)\n",
    "        \n",
    "        if not quiet:\n",
    "            print(f'connection {rand_conn_id} weight modified to {self.connections[rand_conn_id].weight.item():.2f}')\n",
    "\n",
    "    def add_connection(self, quiet):\n",
    "        max_attempts = 100  # prevent infinite loop\n",
    "        for _ in range(max_attempts):\n",
    "            rand_node_in = self.nodes[torch.randint(0, len(self.nodes), (1,)).item()]\n",
    "        \n",
    "            if rand_node_in.is_output:\n",
    "                continue\n",
    "        \n",
    "            rand_node_out = self.nodes[torch.randint(0, len(self.nodes), (1,)).item()]\n",
    "        \n",
    "            if rand_node_out == rand_node_in or rand_node_out.is_input:\n",
    "                continue\n",
    "        \n",
    "            # Skip if connection already exists\n",
    "            # Basically no add connection will work in the beginning because its fully connected\n",
    "            if any(conn.in_node == rand_node_in and conn.out_node == rand_node_out for conn in self.connections):\n",
    "                continue\n",
    "        \n",
    "            # Optional: skip if this would form a cycle\n",
    "            # if self.creates_cycle(rand_node_in, rand_node_out):\n",
    "            #     continue\n",
    "            \n",
    "            if (rand_node_in.id, rand_node_out.id) not in list(NN.global_connections.keys()):\n",
    "                NN.global_connections.update({(rand_node_in.id, rand_node_out.id): 0})\n",
    "            innov_num = list(NN.global_connections.keys()).index((rand_node_in.id, rand_node_out.id))\n",
    "            \n",
    "            conn = ConnectionGene(rand_node_in, rand_node_out, innov_num, torch.randn(1))\n",
    "            self.connections.append(conn)\n",
    "            rand_node_out.num_incoming_connections += 1\n",
    "            \n",
    "            if not quiet:\n",
    "                print(f\"Connection created from node {rand_node_in.id} to node {rand_node_out.id} (innovation #{innov_num})\")\n",
    "\n",
    "            return\n",
    "            \n",
    "        if not quiet:\n",
    "            print(\"Failed to add connection after max attempts.\")\n",
    "\n",
    "    def add_node(self, quiet):            \n",
    "        rand_conn_id = torch.randint(0, len(self.connections), (1,)).item()\n",
    "\n",
    "        while not self.connections[rand_conn_id].enable:\n",
    "            rand_conn_id = torch.randint(0, len(self.connections), (1,)).item()\n",
    "\n",
    "        # Splits an existing connection by adding a node\n",
    "        self.connections[rand_conn_id].enable = False\n",
    "\n",
    "        if (self.connections[rand_conn_id].in_node.id, self.connections[rand_conn_id].out_node.id) in NN.global_connections.keys():\n",
    "            new_node_id = NN.global_connections[(self.connections[rand_conn_id].in_node.id, self.connections[rand_conn_id].out_node.id)]\n",
    "        else:\n",
    "            new_node_id = max(NN.nodes) + 1\n",
    "            NN.global_connections[(self.connections[rand_conn_id].in_node.id, self.connections[rand_conn_id].out_node.id)] = new_node_id\n",
    "            NN.nodes.add(new_node_id)\n",
    "\n",
    "        new_node = Node(new_node_id)\n",
    "        self.nodes.append(new_node)\n",
    "\n",
    "        in_out_tuple = (self.connections[rand_conn_id].in_node.id, new_node.id)\n",
    "        if in_out_tuple not in list(NN.global_connections.keys()):\n",
    "            NN.global_connections.update({in_out_tuple: None}) # No resulting node until it is split for the first time\n",
    "        innov_num = list(NN.global_connections.keys()).index(in_out_tuple)\n",
    "        \n",
    "        conn = ConnectionGene(self.connections[rand_conn_id].in_node, new_node, innov_num, torch.randn(1))\n",
    "        self.connections.append(conn)\n",
    "\n",
    "        in_out_tuple = (new_node.id, self.connections[rand_conn_id].out_node.id)\n",
    "        if in_out_tuple not in list(NN.global_connections.keys()):\n",
    "            NN.global_connections.update({in_out_tuple: None}) # No resulting node until it is split for the first time\n",
    "        innov_num = list(NN.global_connections.keys()).index(in_out_tuple)\n",
    "        \n",
    "        conn = ConnectionGene(new_node, self.connections[rand_conn_id].out_node, innov_num, torch.randn(1))\n",
    "        self.connections.append(conn)\n",
    "        \n",
    "        if not quiet:\n",
    "            print(f'connection {rand_conn_id} split')\n",
    "\n",
    "    def toggle_connection(self, quiet):\n",
    "        rand_conn_id = torch.randint(0, len(self.connections), (1,)).item()\n",
    "\n",
    "        self.connections[rand_conn_id].enable = not self.connections[rand_conn_id].enable\n",
    "\n",
    "        if not quiet:\n",
    "            print(f'connection {rand_conn_id} toggled to {self.connections[rand_conn_id].enable}')\n",
    "\n",
    "    def mutate(self, quiet=False):\n",
    "        # For this experiment i use 70 weight perturbation, 20 weight mutation, 5 add connection, 3 add node, 2 toggle\n",
    "        # Each one is chosen independently of each other\n",
    "        # Does not include crossover. That cannot be done by itself\n",
    "        new_model = self.clone(True)\n",
    "\n",
    "        # Do mutations on new_model\n",
    "        if random.random() < 1:\n",
    "            new_model.weight_perturbation(quiet)\n",
    "        if random.random() < 1:\n",
    "            new_model.weight_modification(quiet)\n",
    "        if random.random() < 1:\n",
    "            new_model.add_connection(quiet)\n",
    "        if random.random() < 1:\n",
    "            new_model.add_node(quiet)\n",
    "        if random.random() < 1:\n",
    "            new_model.toggle_connection(quiet)\n",
    "    \n",
    "        return new_model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a08c669d-7c2d-4b73-993e-c2369ef3a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover(info1, info2):\n",
    "    # Equal Fitness\n",
    "    # Might not implement this rn because it doesnt happen that much\n",
    "    if info1['loss'] == info2['loss']:\n",
    "        pass\n",
    "\n",
    "    # Find fitter model\n",
    "    fit_model, less_fit_model = (info2['model'], info1['model']) if info1['loss'] > info2['loss'] else (info1['model'], info2['model'])\n",
    "\n",
    "    # New model starts off as clone of more fit\n",
    "    new_model = fit_model.clone(True)\n",
    "\n",
    "    fit_conn_pointer = less_fit_conn_pointer = 0\n",
    "\n",
    "    while fit_conn_pointer != len(fit_model.connections) - 1 and less_fit_conn_pointer != len(less_fit_model.connections) - 1:\n",
    "        # IDK\n",
    "        return\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f53ddb4b-c42d-4bb9-aab8-31764af7ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_compatibility(genome1, genome2, c1, c2, c3, delta_thresh):\n",
    "    genome1_conns = {i.innov_num: i.weight for i in genome1.connections}\n",
    "    genome2_conns = {i.innov_num: i.weight for i in genome2.connections}\n",
    "\n",
    "    innovs1 = set(genome1_conns.keys())\n",
    "    innovs2 = set(genome2_conns.keys())\n",
    "\n",
    "    max_innov1 = max(innovs1) if innovs1 else 0\n",
    "    max_innov2 = max(innovs2) if innovs2 else 0\n",
    "    max_innov = max(max_innov1, max_innov2)\n",
    "\n",
    "    # Matching genes: innovation numbers in both genomes\n",
    "    matching = innovs1.intersection(innovs2)\n",
    "    # Calculate average weight difference for matching genes\n",
    "    if matching:\n",
    "        weight_diff = sum(abs(genome1_conns[i] - genome2_conns[i]) for i in matching) / len(matching)\n",
    "    else:\n",
    "        weight_diff = 0\n",
    "\n",
    "    # Excess genes: genes whose innovation number is greater than max innovation number of other genome\n",
    "    excess = 0\n",
    "    for innov in innovs1:\n",
    "        if innov > max_innov2:\n",
    "            excess += 1\n",
    "    for innov in innovs2:\n",
    "        if innov > max_innov1:\n",
    "            excess += 1\n",
    "\n",
    "    # Disjoint genes: genes that do not match and are not excess\n",
    "    disjoint = (len(innovs1 - innovs2) + len(innovs2 - innovs1)) - excess\n",
    "\n",
    "    # Normalization factor N\n",
    "    N = max(len(genome1_conns), len(genome2_conns))\n",
    "    if N < 20:\n",
    "        N = 1  # as per original NEAT paper for small genomes\n",
    "\n",
    "    delta = (c1 * excess / N) + (c2 * disjoint / N) + (c3 * weight_diff)\n",
    "    \n",
    "    # return delta\n",
    "    return True if delta < delta_thresh else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27876d56-916c-4695-b0b7-9cb94b94bd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "top model loss: 55.78\n",
      "epoch: 1\n",
      "top model loss: 54.49\n",
      "epoch: 2\n",
      "top model loss: 53.02\n",
      "epoch: 3\n",
      "top model loss: 50.38\n",
      "epoch: 4\n"
     ]
    }
   ],
   "source": [
    "# \"Training loop\"\n",
    "import math\n",
    "\n",
    "# Hyperparameters\n",
    "population_size = 100\n",
    "epochs = 500\n",
    "input_dim = 64\n",
    "output_dim = 10\n",
    "top_k = 0.2 # The percentage of genomes to keep for reproduction\n",
    "\n",
    "# hyperparameters for measuring compatibility from https://nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf\n",
    "c1 = 1.0\n",
    "c2 = 1.0\n",
    "c3 = 3.0\n",
    "delta_thresh = 4.0\n",
    "\n",
    "# Init\n",
    "# Using list of lists\n",
    "# Dead species will not be kept track of. There will be no empty list\n",
    "population = []\n",
    "\n",
    "for _ in range(population_size):\n",
    "    new_model = {\"model\": NN(input_dim, output_dim), \"loss\": float('inf'), \"fitness\": -float('inf')}\n",
    "\n",
    "    # First model\n",
    "    if len(population) == 0:\n",
    "        population.append([new_model])\n",
    "    else:\n",
    "        for idx, species in enumerate(population):\n",
    "            if measure_compatibility(new_model['model'], species[0]['model'], c1, c2, c3, delta_thresh):\n",
    "                population[idx].append(new_model)\n",
    "            else:\n",
    "                # New species created\n",
    "                population.append([new_model])\n",
    "                \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    \n",
    "    for species in population:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for model_info in species:\n",
    "                model = model_info[\"model\"]\n",
    "                total_loss = 0.0\n",
    "                total_samples = 0\n",
    "    \n",
    "                for data_batch, label_batch in loader:\n",
    "                    output = model(data_batch)\n",
    "                    loss = loss_fn(output, label_batch)\n",
    "                    total_loss += loss.item() * data_batch.size(0)\n",
    "                    total_samples += data_batch.size(0)\n",
    "                \n",
    "                model_info[\"loss\"] = total_loss / total_samples\n",
    "\n",
    "    flattened_population = []\n",
    "\n",
    "    for species in population:\n",
    "        for genome in species:\n",
    "            flattened_population.append(genome)\n",
    "            \n",
    "    ranked_models = sorted([model_info for model_info in flattened_population], key=lambda x: x[\"loss\"])\n",
    "\n",
    "    print(f\"top model loss: {ranked_models[0]['loss']:.2f}\")\n",
    "\n",
    "    # Fitness sharing\n",
    "    for species in population:\n",
    "        species_size = len(species)\n",
    "        for genome in species:\n",
    "            raw_fitness = 1 / (1 + genome['loss'])\n",
    "            genome['fitness'] = raw_fitness / species_size\n",
    "\n",
    "    # This is just a list not a list of lists\n",
    "    new_population = []\n",
    "\n",
    "    for species in population:\n",
    "        offspring = []\n",
    "        \n",
    "        ranked_models = sorted([model_info for model_info in species], key=lambda x: x[\"fitness\"], reverse=True)\n",
    "        parents = [model_info['model'] for model_info in ranked_models[:math.ceil(top_k * len(ranked_models))]]\n",
    "\n",
    "        while len(offspring) != len(ranked_models):\n",
    "            offspring.append({\"model\": random.choice(parents).mutate(True), \"loss\": float('inf'), \"fitness\": -float('inf')})\n",
    "            \n",
    "        new_population.extend(offspring)\n",
    "\n",
    "    # Redivide into species\n",
    "    new_population_divided = []\n",
    "\n",
    "    for model in new_population:    \n",
    "        # First model\n",
    "        if len(new_population_divided) == 0:\n",
    "            new_population_divided.append([model])\n",
    "        else:\n",
    "            for idx, species in enumerate(new_population_divided):\n",
    "                if measure_compatibility(model['model'], species[0]['model'], c1, c2, c3, delta_thresh):\n",
    "                    new_population_divided[idx].append(model)\n",
    "                else:\n",
    "                    # New species created\n",
    "                    new_population_divided.append([model])\n",
    "                    \n",
    "    population = new_population_divided\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852adb2-c82f-47fc-acc9-d65bf7cfb9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a3d1afc7-bc50-4463-a2e9-1dd53fa81dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(64, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5454f1fd-ee56-4bdc-81c8-b1c5b2fd6566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -8.2188, -15.4980, -83.0724,  33.3792, -99.2694,   3.3089,  24.0452,\n",
       "        -37.0231, -81.9885,  47.4248])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fa66cc0e-53b7-49d5-ab65-9d64d27e09d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection 597 weight perturbated to 1.89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NN()"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.mutate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4e3e0-9bbb-4024-a506-dd98805d749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ca278-7301-4bbc-b224-0591355faeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(data_tensor[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neat",
   "language": "python",
   "name": "neat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
